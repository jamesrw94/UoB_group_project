# Evaluation

## Design Prototyping

Our opinion of prototyping was that it was incredibly useful tool to ensure a user-centred design is followed. The prototypes allowed us to involve the user in the design process and get feedback on designs before we started coding the application. We found paper and digital prototyping had different strengths and weaknesses, but they were equally useful in the creation and development of our design.

### Paper Prototype (Completed 2nd of March)

The first development that occurred after ideation was creating a “paper” prototype on Microsoft PowerPoint.

#### Paper Prototype Strengths
1.	Assess ideas quickly and remove elements without investing too much time on them if they didn’t work well.
2.	Quick to produce in comparison to higher-fidelity prototypes
3.	It provided a creative outlet for us to work on as a group
4.	It improved our communication by getting us onto the same page and focused on the same design.

#### Paper Prototype Limitations
1.	People interact with paper differently to actual screens, there is limited context to reflect the actual screen environment.
2.	Limited interactivity/context (tweets/content did not change in our paper prototype)

Despite its limitations paper prototyping played a key part in developing our idea, allowing everyone to easily contribute to the design.  While we were collaboratively working people developed their own versions of pages before we settled on the final design. 

### Digital Prototype (Completed 9th of March)

The limitations of the paper prototype led us to develop a higher-fidelity prototype of our design. 

#### Digital Prototype Strengths
1.	As the prototype was stored online, it could be shared easily with potential users as an URL in the survey itself.
2.	The digital prototype allowed us to simulate most of the interactions between the user and the application, providing us with a lot more information about their interactions.
3.	We received richer feedback using user testing, allowing more informed, user-centred decisions to be made.
4.	Able to easily perform quantitative user surveys


#### Digital Prototype Limitations
1.	It was a lot more time consuming, than paper prototyping.
2.	The platform we used could not fully recreate the application we desired to create.


If given more time, an even higher prototype would have been useful to implement on a platform which had drag and drop.  Despite searching, a better platform could not be found during the design phase of the project.  However **[inVision](https://www.invisionapp.com/)** was found later, which looks to be a tool that could have supported drag and drop, allowing an even higher-prototype to be performed.

## User Tests

### User Survey (Released 22nd of March)

The survey included pre user test questions on demographics such as age, gender, profession and location. More targeted questions were then asked regarding their knowledge of newspapers.  Once they had performed the user test, they were asked questions related to the design and purpose of the website.

The hope was to be able to gain insights into our target demographic, to be able to improve our design for their user needs.  Several insights were gleaned from the survey which had a direct impact on the design:

1.	The survey was incredibly useful in assigning a name to our application, something we were struggling with a decision on.  
2.	It allowed us to understand that people were having difficulties understanding the concepts within the game which led the front end to adding subtitles to add clarity.
3.	A lot of users were using their mobile phone to test the prototype, which led to a discussion regarding mobile compatibility of the website. 
4.	The fact that the prototype did not have drag and drop features confused a lot of users.

For a full lists of insights please check the **[User Report]( https://github.com/jamesrw94/UoB_group_project/blob/main/UX_Design/Report.pptx)**.  

Reflecting on the questions asked within the survey, there could have been some improvement in this area, which could have generated more detailed insights for example:

> “Now you have played, do you think it has improved your perception of bias in the media?”

and 

> ”Did you manage to do what you wanted on the website?”

These questions were asked too early and should have been used once a fully coded prototype was available.  The prototype they were reviewing had limited functionality, so it makes sense that it would not be as insightful to ask at this time.

Despite the useful information we gained from user surveys, we found there were some limitations to its method:

1.	We could not study user behaviour on the site, to see when they are pausing or where exactly they are getting stuck.
2.	Preparing questions took quite a lot of time to ensure they were worded correctly, ensuring bias and leading questions were removed to the best of our ability.
3.	Difficult to establish what usability problems there are with the product.

The insights (Insight 4 specifically) from the user survey led us to creating a moderated user test, as we felt we wanted more qualitative data to help drive our design forward.

### Moderated User Test (Performed 1st of April / 2nd of April)

During moderated user testing you have the chance to observe the user operating the prototype which can give very different and more comprehensive insights than the user survey provided.  It also provided a platform to allow me to clarify any confusion the user had during the user test, directing them away from features that do not work as intended.

The moderated user test was performed over Zoom – With the participants sharing their screen so I could observe them using the application.  The test was semi-structured, I gave the users a task (play one Brexit question, one Politics question and finally a random question).  While the user was operating the application, if I felt they paused at any point I would probe them for information on why they paused.

Although there was quite a lot of useful information, I made a decision to make something an insight if both users mentioned it during their tasks, otherwise it could just be one user’s opinion.  The insights were:

1.	Both users commented on the “Have I got News for You” theme, from that they thought it was a game or quiz, so it had the intended outcome.
2.	They confirmed the name choice of the application as “Know the News”.
3.	Users commented that they wanted a bit of an explanation of what to do on each component:

> “ I probably expect a bit of an explanation below the title… however it is obvious I am supposed  to click these.”

> “I think from my first impressions that this is a game… I’d be familiar enough to click on the buttons however a bit of an explanation would be best to go with it, so you know how many to pick.”

With the final insight, a mini A/B test was carried out to test two variations of the answer component.  One had two tweets to compare and match up while the second and only one tweet, but two newspapers to choose from.

<p align="center">
  <img src="https://raw.githubusercontent.com/jamesrw94/UoB_group_project/main/Evaluation/ABTest.png">
</p>

The users preferred having two tweets, as it gave them more of a chance to pick between the two:

> “I think that one (two tweets) is easier… having one tweet was OK, I mean having two to compare, I suppose if you aren’t too sure, then you’ve got a second chance of realizing which one is which as one might be easier to decide / get it right?”

> “I think it is more obvious that you are incorrect (with 2) however would still be nice to see both tweets”

#### Limitations
1.	Planning, organizing and executing the user tests is time consuming, and a lot more hands on than user surveys.
2.	Documenting the results also is time intensive as the videos had to be replayed to gain quotes, afterwards comparing the user journeys to gain insights.
3.	Moderating bias/errors could have a significant impact on results as the sample size is much lower.

On the whole, user prototyping and testing was an incredibly valuable tool for us to ensure the application would satisfy the user needs. I feel that using both methods complemented each other. Many useful insights were highlighted by both methods which were later used in the design by the front and back end.



## Unit testing / Functional testing



## User acceptance testing

### Methods

### Findings

### Implications


**[&rarr; Conclusion](https://github.com/jamesrw94/UoB_group_project/blob/main/Conclusion/README.md)**

**[&larr; Back to Index](https://github.com/jamesrw94/UoB_group_project)**
